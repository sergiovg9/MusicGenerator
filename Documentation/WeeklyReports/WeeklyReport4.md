# Weekly report 4

This week’s work focused on researching log-likelihood as a method for testing Markov models and integrating it into the full workflow of training and generation. Four main components were implemented: a training script to build Markov models of different orders from note sequences; a validation script that computes log-likelihood on a validation set to determine which model order performs best; a testing script that evaluates the selected model using unseen data; and finally a Markov generator, capable of producing new melodies given a seed, an order, and a musical key, with automatic transposition and weighted sampling.

`training.py` loads collections of token sequences from JSON files and uses them to train Markov chain models of different orders. It first scans a specified directory, extracts the “tokens” list from each file, and gathers all valid sequences. Then, for each desired Markov order, it iterates through every sequence to count how often each possible n-gram (state) is followed by a given next token. These counts are converted into transition probabilities, producing a complete Markov model for that order.

After training, the script saves each model in JSON format, converting tuple-based states into comma-separated strings to ensure compatibility with JSON. The overall pipeline automatically trains and exports Markov models of order 1 through 4, storing each one in the models directory for later use in music-generation or sequence-prediction tasks.

Compared to `markov_model.py` from the previous version, this version is more general and supports arbitrary Markov orders rather than being limited to first-order transitions. It also focuses solely on training and saving models (without sequence generation), loads only training sequences instead of multiple dataset splits, and stores models in a structured format that encodes high-order states.

`validation.py` evaluates the previously trained Markov chain models using validation data. It begins by loading all token sequences from the validation folder and then loads each Markov model (orders 1–4). For every sequence, it computes the **log-likelihood** by walking through the tokens, checking whether each state–transition pair exists in the model, and adding either the logarithm of its probability or a large negative penalty if the transition is unknown. The script averages these log-likelihoods across all validation sequences to measure how well each order predicts unseen data. According to the output produced during evaluation, **the best-performing order is 1**.

`testing.py` loads a trained Markov model and evaluates its performance on a set of test sequences. It first reconstructs the model by converting each string-encoded state back into a tuple, then loads all token sequences from the test directory. The script automatically infers the order of the model by inspecting the length of its state keys.

For each test sequence, it calculates the **log-likelihood** by sliding a window of size equal to the model order and checking the probability of each next token. Unknown states or transitions are handled by assigning a very small fallback probability. After computing all individual log-likelihoods, the script averages them to obtain the final score over the entire test set. In this run, even though the validation step suggested that order 1 had the highest likelihood, the script evaluates an order-2 model because first-order generation subjectively sounded too random.

`markov_generator.py` generates note sequences using the Markov models stored. Its main functionalities include **maintaining a global cache** so each model (order 1–4) is loaded only once, and **normalizing input tonalities** by transposing any key to C major / A minor using a `KEY_TO_SEMITONES` map, later reversing that transposition for the final output. It also validates all user inputs (order, seed length, number of measures, and key) and converts measures into a total number of notes (1 measure = 4 notes).

During generation, the script takes the seed (transposed), forms the initial state as a tuple of length equal to the Markov order, and enters a loop where it queries the model's transition probabilities, selects the next note using weighted sampling, stops if it encounters the "END" token or an unseen state, and updates the sliding window. Utility functions handle transposition of individual notes and full sequences, as well as weighted choice. The final generated sequence is then transposed back to the user’s original key.

A testing file named `test_generator.py` was created to interact with this generator from the command line. It asks the user for the Markov order, number of measures, key, and the required number of seed notes. With those parameters, it prints the resulting sequence, or displays an error message if invalid input or missing models cause an exception, emulating the UI app.