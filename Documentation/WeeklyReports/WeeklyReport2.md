# Weekly report 2

Research was made to understand better how MIDI files work, how they store musical performance data, and how they can be processed for use in machine learning models. During this stage, the focus was on learning how Markov chains could be applied to generate music and how to overcome their limitations when handling features like pitch and duration separately. The solution was to represent music as a single chain of connected events, where notes, time shifts, and velocity changes are all part of one continuous sequence. This approach allows the model to capture how rhythm, timing, and harmony interact in real piano performances.

The data collection step was designed to automatically download and organize the MAESTRO dataset for local use, so the files wouldn’t need to be uploaded to GitHub due to their large size. The script downloaded the dataset directly from MAESTRO website, extracted all its contents into a clean folder structure inside the data/ directory, and grouped the MIDI files into one place for easy access. This setup made it simple to reproduce the workflow and let anyone use the dataset without manual setup.

During preprocessing, all MIDI files were read with the music21 library to extract detailed musical information. Each piece was turned into a sequence of symbolic tokens like NOTE_ON, NOTE_OFF, VELOCITY, and TIME_SHIFT. The time resolution was set to 0.25 beats, meaning each TIME_SHIFT_1 equals a 1/16 note, and note velocities were divided into eight bins to simplify how dynamics are represented. To improve processing speed, several optimizations were applied: files were accessed using the dataset’s CSV index rather than scanning directories recursively, sequences were built using vectorized operations where possible, and unnecessary parsing of audio data was avoided. These changes significantly reduced processing time per file, making it practical to handle hundreds of MIDI recordings efficiently.

The preprocessing keeps polyphony, meaning that, if several notes are played at the same time, they’re represented as multiple NOTE_ON events with the same timing, not as one chord token. Note durations aren’t stored directly but can be calculated by the number of time-shift steps between NOTE_ON and NOTE_OFF. This method keeps the expressive and timing details of real piano performances while staying compatible with sequence-based models like Markov chains.